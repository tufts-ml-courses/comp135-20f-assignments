{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day23: Introduction to autograd for gradient descent\n",
    "\n",
    "\n",
    "# Objectives\n",
    "\n",
    "We will practice:\n",
    "\n",
    "* Using the **autograd** Python package to compute gradients of functions\n",
    "* Using gradients from autograd to do a basic linear regression\n",
    "\n",
    "# Outline\n",
    "\n",
    "* [Part 1: Autograd for scalar input, scalar output functions](#part1)\n",
    "* [Part 2: Autograd for vector input, scalar output functions](#part2)\n",
    "* [Part 3: Using autograd with simple gradient descent procedure](#part3)\n",
    "* [Part 4: Using autograd to solve linear regression](#part4)\n",
    "* [Part 5: Using autograd with functions that take dicts and other data structures](#part5)\n",
    "\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "* Automatic differentiation is a powerful idea that has made experimenting with different models and loss functions far easier than it was even 8 years ago.\n",
    "* The Python package `autograd` is a wonderfully simple tool that makes this work with numpy/scipy\n",
    "\n",
    "* `autograd` works by a super-smartly implemented version of the backpropagation dynamic programming we've already discussed from Unit 3\n",
    "* * Basically, after doing a \"forward\" pass to evaluate the function, we do a \"reverse\" pass through the computation graph and compute gradients via the chain rule.\n",
    "* * This general purpose method is called [reverse-mode differentiation](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#reverse-mode-differentiation)\n",
    "\n",
    "* `autograd` does NOT do symbolic math!\n",
    "* * e.g. It does not simplify `ag_np.sqrt(ag_np.square(x))` as `x`. It will use the chain rule on all nested functions that the user specifies.\n",
    "* `autograd` does NOT do numerical approximations to gradients.\n",
    "* * e.g. It does not estimate gradients by perturbing inputs slightly\n",
    "\n",
    "* In Part 5, we see how we can define losses in terms of dictionaries, which let us define complicated models with many different parameters. We'll exploit this in Project C for matrix factorization with many parameters\n",
    "\n",
    "# Limitations\n",
    "\n",
    "FYI There are some things that autograd *cannot* handle that you should be aware of. \n",
    "\n",
    "Make sure any loss function you define that you want to differentiate does not do any of these things:\n",
    "\n",
    "* Do not use assignment to elements of arrays, like `A[0] = x` or `A[1] = y`\n",
    "* * Instead, compute entries individually and then stack them together.\n",
    "* * Like this: `x = ...; y = ...; A = ag_np.hstack([x, y])`\n",
    "* Do not rely on implicit casting of lists to arrays, like `A = ag_np.sum([x, y])`\n",
    "* * use `A = ag_np.sum(ag_np.array([x, y]))` instead.\n",
    "* Do not use A.dot(B) notation\n",
    "* * Instead, use `ag_np.dot(A, B)`\n",
    "* Avoid in-place operations (such as `a += b`)\n",
    "* * Instead, use a = a + b\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "Check out these great resources\n",
    "\n",
    "* Official tutorial for the autograd package: https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
    "* Short list of what autograd *can* and *cannot* do: https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#supported-and-unsupported-parts-of-numpyscipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import autograd\n",
    "import autograd.numpy as ag_np\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Using autograd.grad for univariate functions\n",
    "\n",
    "Suppose we have a mathematical function of interest $f(x)$.\n",
    "\n",
    "For now, we'll assume this function has a scalar input and scalar output. This means:\n",
    "\n",
    "* $x \\in \\mathbb{R}$\n",
    "* $f(x) \\in \\mathbb{R}$\n",
    "\n",
    "We can ask: what is the derivative (aka *gradient*) of this function:\n",
    "\n",
    "$$\n",
    "g(x) \\triangleq \\frac{\\partial}{\\partial x} f(x)\n",
    "$$\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use `autograd` to do it for us.\n",
    "\n",
    "First, we need to implement the math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': scalar float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy\n",
    "\n",
    "### From numpy to autograd's wrapper of numpy\n",
    "\n",
    "You might be used to importing numpy as `import numpy as np`, and then using this shorthand for `np.cos(0.0)` or `np.square(5.0)` etc.\n",
    "\n",
    "For autograd to work, you need to instead use **autograd's** provided numpy wrapper interface:\n",
    "\n",
    "`from autograd.numpy as ag_np`\n",
    "\n",
    "The `ag_np` module has the same API as `numpy`. So for example, you can call\n",
    "\n",
    "* `ag_np.cos(0.0)`\n",
    "* `ag_np.square(5.0)`\n",
    "* `ag_np.sum(a_N)`\n",
    "* `ag_np.mean(a_N)`\n",
    "* `ag_np.dot(u_NK, v_KM)`\n",
    "\n",
    "Or almost any other function you usually would use with `np`\n",
    "\n",
    "**Summary:** Make sure your function `f` produces a scalar and only uses functions within the `ag_np` wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: f(x) = x^2\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return ag_np.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients with autograd\n",
    "\n",
    "Given a Python function `f` that meets our requirements and evaluates $f(x)$, we want a Python function ``g` that computes the gradient $g(x) \\triangleq \\frac{\\partial}{\\partial x}$\n",
    "\n",
    "We can use `autograd.grad` to create a Python function `g` \n",
    "\n",
    "```\n",
    "g = autograd.grad(f) # create function g that produces gradients of input function f\n",
    "```\n",
    "\n",
    "The symbol `g` is now a **Python function** that takes the same input as `f`, but produces the derivative at a given input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'g' is just a function.\n",
    "# You can call it as usual, by providing a possible scalar float input\n",
    "\n",
    "g(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1a: Do you agree that the printed values above are correct? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to demonstrate the gradient function  side-by-side with original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values evenly spaced between -5 and 5\n",
    "x_grid_G = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, squeeze=False)\n",
    "subplot_grid[0,0].plot(x_grid_G, [f(x_g) for x_g in x_grid_G], 'k.-')\n",
    "subplot_grid[0,0].set_title('f(x) = x^2')\n",
    "\n",
    "subplot_grid[0,1].plot(x_grid_G, [g(x_g) for x_g in x_grid_G], 'b.-')\n",
    "subplot_grid[0,1].set_title('gradient of f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b:\n",
    "\n",
    "Consider the decaying periodic function below. Can you compute its derivative using autograd and plot the result?\n",
    "\n",
    "$$\n",
    "f(x) = e^{-x/10} * cos(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.0 # TODO compute the function above, using 'ag_np'\n",
    "    \n",
    "g = f # TODO define g as gradient of f, using autograd's `grad` \n",
    "\n",
    "# TODO plot the result\n",
    "x_grid_G = np.linspace(-10, 10, 500)\n",
    "fig_h, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, squeeze=False)\n",
    "subplot_grid[0,0].plot(x_grid_G, [f(x_g) for x_g in x_grid_G], 'k.-');\n",
    "subplot_grid[0,0].set_title('f(x) = x^2');\n",
    "\n",
    "subplot_grid[0,1].plot(x_grid_G, [g(x_g) for x_g in x_grid_G], 'b.-');\n",
    "subplot_grid[0,1].set_title('gradient of f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Using autograd.grad for functions with multivariate input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine the input $x$ could be a vector of size D. \n",
    "\n",
    "Our mathematical function $f(x)$ will map each input vector to a scalar.\n",
    "\n",
    "We want the gradient function\n",
    "\n",
    "\\begin{align}\n",
    "g(x) &\\triangleq \\nabla_x f(x)\n",
    "\\\\\n",
    "&= [\n",
    "    \\frac{\\partial}{\\partial x_1} f(x)\n",
    "    \\quad \\frac{\\partial}{\\partial x_2} f(x)\n",
    "    \\quad \\ldots \\quad \\frac{\\partial}{\\partial x_D} f(x)  ]\n",
    "\\end{align}\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use autograd to do it for us.\n",
    "\n",
    "First, we implement math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': numpy array of float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 2a\n",
    "\n",
    "Let's set up a function that is defined as the inner product of the input vector x with some weights $w$\n",
    "\n",
    "We assume both $x$ and $w$ are $D$ dimensional vectors\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{d=1}^D x_d w_d\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fixed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "\n",
    "w_D = np.asarray([1., 2.,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function `f` using `ag_np` wrapper functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.dot(x_D, w_D) # dot product is just inner product in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `autograd.grad` to get the gradient function `g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try putting in the all-zero vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.zeros(D)\n",
    "\n",
    "print(\"x_D\", x_D)\n",
    "print(\"f(x_D) = %.3f\" % (f(x_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient wrt that all-zero vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.asarray([1., 2.])\n",
    "\n",
    "print(\"x_D\", x_D)\n",
    "print(\"f(x_D) = %.3f\" % (f(x_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient wrt the vector [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2b: Does this gradient computation agree with what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "\n",
    "Let's set up a function that is just the sum-of-squares penalty on the input vector x\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{d=1}^D x_d^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function `f` using `ag_np` wrapper functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return 0.0 # TODO define sum-of-squares function f via calls to ag_np functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `autograd.grad` to get the gradient function `g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = None # TODO fixme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out an input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.asarray([1., 2.])\n",
    "\n",
    "print(\"x_D\", x_D)\n",
    "print(\"f(x_D) = %.3f\" % (f(x_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2d: Mathematically, should the gradient of sum-of-squares function be when x is all zero? Does your function `g` agree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO try out feeding `np.zeros(D)` into your `g` function and discuss if you get what you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using autograd gradients within gradient descent to solve multivariate optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: basic gradient descent\n",
    "\n",
    "Here's a very simple function that will perform many gradient descent steps to optimize a given function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many_iters_of_gradient_descent(f, g, init_x_D=None, n_iters=100, step_size=0.001):\n",
    "    ''' Run many iterations of GD\n",
    "    \n",
    "    Args\n",
    "    ---- \n",
    "    f : python function (D,) to float\n",
    "        Maps vector x_D to scalar loss\n",
    "    g : python function, (D,) to (D,)\n",
    "        Maps vector x_D to gradient g_D\n",
    "    init_x_D : 1D array, shape (D,)\n",
    "        Initial value for the input vector\n",
    "    n_iters : int\n",
    "        Number of gradient descent update steps to perform\n",
    "    step_size : positive float\n",
    "        Step size or learning rate for GD\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_D : 1D array, shape (D,)\n",
    "        Best value of input vector for provided loss f found via this GD procedure\n",
    "    history : dict\n",
    "        Contains history of this GD run useful for plotting diagnostics\n",
    "    '''\n",
    "    # Copy the initial parameter vector\n",
    "    x_D = copy.deepcopy(init_x_D)\n",
    "\n",
    "    # Create data structs to track the per-iteration history of different quantities\n",
    "    history = dict(\n",
    "        iter=[],\n",
    "        f=[],\n",
    "        x_D=[],\n",
    "        g_D=[])\n",
    "\n",
    "    for iter_id in range(n_iters):\n",
    "        if iter_id > 0:\n",
    "            x_D = x_D - step_size * g(x_D)\n",
    "\n",
    "        history['iter'].append(iter_id)\n",
    "        history['f'].append(f(x_D))\n",
    "        history['x_D'].append(x_D)\n",
    "        history['g_D'].append(g(x_D))\n",
    "    return x_D, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 3a: Minimize f(x) = sum(square(x))\n",
    "\n",
    "It's easy to figure out that the vector with smallest L2 norm (smallest sum of squares) is the all-zero vector.\n",
    "\n",
    "Here's a quick example of showing that using gradient functions provided by autograd can help us solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_x  \\sum_{d=1}^D x_d^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.sum(ag_np.square(x_D))\n",
    "\n",
    "g = autograd.grad(f)\n",
    "\n",
    "# Initialize at x_D = [6, 4, -3, -5]\n",
    "D = 4\n",
    "init_x_D = np.asarray([6.0, 4.0, -3.0, -5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x_D, history = run_many_iters_of_gradient_descent(f, g, init_x_D, n_iters=1000, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots of how x parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: f goes to zero. all x values goto zero.\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "for d in range(D):\n",
    "    subplot_grid[0,0].plot(history['iter'], np.vstack(history['x_D'])[:,d], label='x[%d]' % d);\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('x_d')\n",
    "subplot_grid[0,0].legend(loc='upper right')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3b: Minimize the 'trid' function\n",
    "\n",
    "Given a 2-dimensional vector $x = [x_1, x_2]$, the trid function is:\n",
    "\n",
    "$$\n",
    "f(x) = (x_1-1)^2 + (x_2-1)^2 - x_1 x_2\n",
    "$$\n",
    "\n",
    "Background and Picture: <https://www.sfu.ca/~ssurjano/trid.html>\n",
    "\n",
    "Can you use autograd + gradient descent to find the optimal value $x^*$ that minimizes $f(x)$?\n",
    "\n",
    "You can initialize your gradient descent at [+1.0, -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return 0.0 # TODO\n",
    "\n",
    "g = f # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call run_many_iters_of_gradient_descent() with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRID example\n",
    "# Make plots of how x parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: ????\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "for d in range(D):\n",
    "    subplot_grid[0,0].plot(history['iter'], np.vstack(history['x_D'])[:,d], label='x[%d]' % d);\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('x_d')\n",
    "subplot_grid[0,0].legend(loc='upper right')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Solving linear regression with gradient descent + autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe $N$ examples $(x_n, y_n)$ consisting of D-dimensional 'input' vectors $x_n$ and scalar outputs $y_n$.\n",
    "\n",
    "Consider the multivariate linear regression model for making a prediction given any input vector $x_i \\in \\mathbb{R}^D$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(x_i) = w^T x_i\n",
    "\\end{align}\n",
    "\n",
    "One way to train weights would be to just compute the weights that minimize mean squared error\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{w \\in \\mathbb{R}^D}  \\sum_{n=1}^N (y_n - x_n^T w )^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data for linear regression task\n",
    "\n",
    "We'll generate data that comes from an idealized linear regression model.\n",
    "\n",
    "Each example has D=2 dimensions for x.\n",
    "\n",
    "* The first dimension is weighted by +4.2.\n",
    "\n",
    "* The second dimension is weighted by -4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2\n",
    "sigma = 0.1\n",
    "\n",
    "true_w_D = np.asarray([4.2, -4.2])\n",
    "true_bias = 0.1\n",
    "\n",
    "train_prng = np.random.RandomState(0)\n",
    "x_ND = train_prng.uniform(low=-5, high=5, size=(N,D))\n",
    "y_N = np.dot(x_ND, true_w_D) + true_bias + sigma * train_prng.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data Visualization: Pairplots for all possible (x_d, y) combinations\n",
    "\n",
    "You can clearly see the slopes of the lines:\n",
    "* x1 vs y plot: slope is around +4\n",
    "* x2 vs y plot: slope is around -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=pd.DataFrame(np.hstack([x_ND, y_N[:,np.newaxis]]), columns=['x1', 'x2', 'y']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem as an AUTOGRAD-able function wrt the weights w_D\n",
    "def calc_squared_error_loss(w_D):\n",
    "    return ag_np.sum(ag_np.square(ag_np.dot(x_ND, w_D) - y_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the *loss function* at the known \"ideal\" initial point\n",
    "\n",
    "calc_squared_error_loss(true_w_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createa an all-zero weight array to use as our initial guess\n",
    "\n",
    "init_w_D = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the *loss function* at that all-zero initial point\n",
    "\n",
    "calc_squared_error_loss(init_w_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autograd.grad to build the gradient function\n",
    "\n",
    "calc_grad_wrt_w = autograd.grad(calc_squared_error_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient function at that same initial point \n",
    "\n",
    "calc_grad_wrt_w(init_w_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4a: Is the gradient pointing in the right direction? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradient descent\n",
    "\n",
    "Use the code below to run GD on our simple regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the gradient's magnitude is very large, use very small step size\n",
    "opt_w_D, history = run_many_iters_of_gradient_descent(\n",
    "    calc_loss, calc_grad_wrt_w, init_w_D,\n",
    "    n_iters=400, step_size=0.00001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinReg worked example\n",
    "# Make plots of how w_D parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: x\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "for d in range(D):\n",
    "    subplot_grid[0,0].plot(history['iter'], np.vstack(history['x_D'])[:,d], label='w[%d]' % d);\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('w_d')\n",
    "subplot_grid[0,0].legend(loc='upper right')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('-1 * log p(y | w, x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4b: Do these trace plots indicate we have converged to good weight vector values? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Autograd for functions of data structures of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Fact: autograd can take derivatives with respect to DATA STRUCTURES of parameters\n",
    "\n",
    "This can help us when it is natural to define models in terms of several parts (e.g. NN layers).\n",
    "\n",
    "We don't need to turn our many model parameters into one giant weights-and-biases vector. We can express our thoughts more naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: gradient of a LIST of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w_list_of_arr):\n",
    "    return ag_np.sum(ag_np.square(w_list_of_arr[0])) + ag_np.sum(ag_np.square(w_list_of_arr[1]))\n",
    "\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list_of_arr = [np.zeros(3), np.arange(5, dtype=np.float64)]\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(w_list_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(w_list_of_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: gradient of DICT of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(dict_of_arr):\n",
    "    return ag_np.sum(ag_np.square(dict_of_arr['weights'])) + ag_np.sum(ag_np.square(dict_of_arr['bias']))\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_arr = dict(weights=np.arange(5, dtype=np.float64), bias=4.2)\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(dict_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(dict_of_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5a: Try to implement gradient descent for linear regression with weights and bias parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example only uses weights on the dimensions of each $x$ vector , and thus can only learn linear models that pass through the origin.\n",
    "\n",
    "Can you instead optimize a model that includes a **bias** parameter $b>0$?\n",
    "\n",
    "The predictions look like:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(x_i) = w^T x_i + b\n",
    "\\end{align}\n",
    "\n",
    "The training problem is:\n",
    "    \n",
    "\\begin{align}\n",
    "\\min_{w \\in \\mathbb{R}^D,b \\in \\mathbb{R}} \\sum_{n=1}^N (y_n - w^T x_n - b)^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this format of parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_dict = dict(\n",
    "    w_D=np.zeros(D),\n",
    "    b=1.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5a TODO: Define a function called `calc_loss` that computes our training loss given a parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(param_dict):\n",
    "    w_D = param_dict['w_D'] # Unpack weight array\n",
    "    b = param_dict['b'] # Unpack bias scalar\n",
    "    \n",
    "    return 0.0 # TODO fix me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5a TODO: build function `g` that can compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = None # TODO fix me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: use the implementation below of gradient descent on a parameter dictionary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many_iters_of_gradient_descent_for_param_dict(f, g, init_param_dict=None, n_iters=100, step_size=0.001):\n",
    "    ''' Run many iterations of GD\n",
    "    \n",
    "    Args\n",
    "    ---- \n",
    "    f : python function of dict to float\n",
    "        Maps dict of arrays to scalar loss\n",
    "    g : python function of dict to dict\n",
    "        Maps dict of arrays to gradient dict of arrays\n",
    "    init_param_dict : dict\n",
    "        Initial values for the input parameters\n",
    "    n_iters : int\n",
    "        Number of gradient descent update steps to perform\n",
    "    step_size : positive float\n",
    "        Step size or learning rate for GD\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    opt_param_dict : dict\n",
    "        Best value of parameter dict for provided loss f found via this GD procedure\n",
    "    history : dict\n",
    "        Contains history of this GD run useful for plotting diagnostics\n",
    "    '''\n",
    "    # Copy the initial parameter dict\n",
    "    param_dict = copy.deepcopy(init_param_dict)\n",
    "\n",
    "    # Create data structs to track the per-iteration history of different quantities\n",
    "    history = dict(\n",
    "        iter=[],\n",
    "        f=[],\n",
    "        param_dict=[],\n",
    "        grad_dict=[])\n",
    "\n",
    "    for iter_id in range(n_iters):\n",
    "        if iter_id > 0:\n",
    "            grad_dict = g(param_dict)\n",
    "            for key in param_dict.keys():\n",
    "                p_arr = param_dict[key] # current param array\n",
    "                g_arr = grad_dict[key]  # current gradient array\n",
    "                p_arr = p_arr - step_size * g_arr\n",
    "                param_dict[key] = p_arr # store as latest value in dictionary\n",
    "\n",
    "        history['iter'].append(iter_id)\n",
    "        history['f'].append(f(param_dict))\n",
    "        history['param_dict'].append(param_dict)\n",
    "        history['grad_dict'].append(g(param_dict))\n",
    "    return param_dict, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO Run gradient descent to see what you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
