{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day17: Bag of Words Representations for Text Data\n",
    "\n",
    "# Objectives\n",
    "\n",
    "* Understand bag of words representations\n",
    "* Think through the decision decisions you need to make and how they will impact classifier performance\n",
    "\n",
    "\n",
    "# Outline\n",
    "\n",
    "* [Part 1: Cleaning text into a list of tokens](#part1)\n",
    "* [Part 2: Building a fixed-size vocabulary](#part2)\n",
    "* [Part 3: Creating a BoW feature vector](#part3)\n",
    "* [Part 4: Building a classifier given your BoW features](#part4)\n",
    "* [Part 5: Sklearn's CountVectorizer for easy BoW feature processing](#part5)\n",
    "* [Part 6: Using a pipeline with CountVectorizer](#part6)\n",
    "\n",
    "We expect you'll get through part 4 during this class period. \n",
    "\n",
    "# Takeaways\n",
    "\n",
    "* Bag of words representations are simple and still interpretable\n",
    "* Bag of words representations are limited: you lose any information that comes from the *ordering* of the words\n",
    "* Many key design decisions (how to handle rare words, how to handle similar words like \"walk\" and \"walking\") can matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Raw Text Data\n",
    "\n",
    "We've included some raw text from 200 negative reviews and 200 positive reviews below.\n",
    "\n",
    "This is a subset of the training set you'll use for Project B.\n",
    "\n",
    "Each line is one plain-text review. You'll see many slang terms, weird capitalization/punctuation/etc.\n",
    "\n",
    "Just execute the cell and move on. You'll need to scroll down a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_as_line_separated_string = \"\"\"Oh and I forgot to also mention the weird color effect it has on your phone.\n",
    "THAT one didn't work either.\n",
    "Waste of 13 bucks.\n",
    "Product is useless, since it does not have enough charging current to charge the 2 cellphones I was planning to use it with.\n",
    "None of the three sizes they sent with the headset would stay in my ears.\n",
    "Worst customer service.\n",
    "The Ngage is still lacking in earbuds.\n",
    "It always cuts out and makes a beep beep beep sound then says signal failed.\n",
    "the only VERY DISAPPOINTING thing was there was NO SPEAKERPHONE!!!!\n",
    "Very disappointed in AccessoryOne.\n",
    "Basically the service was very bad.\n",
    "Bad Choice.\n",
    "The only thing that disappoint me is the infra red port (irda).\n",
    "horrible, had to switch 3 times.\n",
    "It feels poorly constructed, the menus are difficult to navigate, and the buttons are so recessed that it is difficult to push them.\n",
    "Don't make the same mistake I did.\n",
    "Muddy, low quality sound, and the casing around the wire's insert was poorly super glued and slid off.\n",
    "I advise EVERYONE DO NOT BE FOOLED!\n",
    "Doesn't hold charge.\n",
    "What a waste of time!\n",
    "I'm very disappointed with my decision.\n",
    "I also didn't like the \"on\" button, it felt like it would crack with use.\n",
    "I bought these hoping I could make my Bluetooth headset fit better but these things made it impossible to wear.\n",
    "We have tried 2 units and they both failed within 2 months.. Pros\n",
    "Also difficult to put on.I'd recommend avoiding this product.\n",
    "$50 Down the drain.\n",
    "Absolutel junk.\n",
    "Can't store anything but phone numbers to SIM.\n",
    "Very disappointing.\n",
    "I would not recommend this item to anyone.\n",
    "Big Disappointment with calendar sync.\n",
    "Not impressed.\n",
    "Just does not work.\n",
    "I even fully charged it before I went to bed and turned off blue tooth and wi-fi and noticed that it only had 20 % left in the morning.\n",
    "Plus, I seriously do not believe it is worth its steep price point.\n",
    "In my house I was getting dropped coverage upstairs and no coverage in my basement.\n",
    "The phone takes FOREVER to charge like 2 to 5 hours literally.\n",
    "Very unreliable service from T-mobile !\n",
    "[...] down the drain because of a weak snap!\n",
    "This is a simple little phone to use, but the breakage is unacceptible.\n",
    "Pretty piece of junk.\n",
    "This is so embarassing and also my ears hurt if I try to push the ear plug into my ear.\n",
    "Unfortunately the ability to actually know you are receiving a call is a rather important feature and this phone is pitiful in that respect.\n",
    "This is the first phone I've had that has been so cheaply made.\n",
    "Awkward to use and unreliable.\n",
    "Horrible phone.\n",
    "If you are looking for a good quality Motorola Headset keep looking, this isn't it.\n",
    "My father has the V265, and the battery is dying.\n",
    "After a year the battery went completely dead on my headset.\n",
    "Defective crap.\n",
    "Poor Construction.\n",
    "They work about 2 weeks then break.\n",
    "Could not get strong enough signal.\n",
    "I really wanted the Plantronics 510 to be the right one, but it has too many issues for me.The good\n",
    "Excellent starter wireless headset.\n",
    "Performed awful -- muffled, tinny incoming sound and severe echo for those on the other end of the call.\n",
    "BT50 battery junk!.\n",
    "The design might be ergonomic in theory but I could not stand having these in my ear.\n",
    "camera color balance is AWFUL.\n",
    "It dit not work most of the time with my Nokia 5320.\n",
    "Looks good in the picture, but this case was a huge disappointment!!\n",
    "I've had this bluetoooth headset for some time now and still not comfortable with the way it fits on the ear.\n",
    "Plug was the wrong size.\n",
    "the phone was unusable and was not new.\n",
    "If I take a picture, the battery drops a bar, and starts beeping, letting me know its dieing.\n",
    "It's so stupid to have to keep buying new chargers, car chargers, cradles, headphones and car kits every time a new phone comes out.\n",
    "poor quality and service.\n",
    "Poor product.\n",
    "I'm a bit disappointed.\n",
    "I tried talking real loud but shouting on the telephone gets old and I was still told it wasn't great.\n",
    "There's a horrible tick sound in the background on all my calls that I have never experienced before.\n",
    "The design is very odd, as the ear \"clip\" is not very comfortable at all.\n",
    "At first I thought I was grtting a good deal at $7.44, until I plugged it into my phone (V3c Razr).\n",
    "dont buy it.\n",
    "So there is no way for me to plug it in here in the US unless I go by a converter.\n",
    "Soyo technology sucks.\n",
    "doesn't last long.\n",
    "Still Waiting...... I'm sure this item would work well.. if I ever recieve it!\n",
    "Poorly contstruct hinge.\n",
    "Think it over when you plan to own this one!This sure is the last MOTO phone for me!\n",
    "Problem is that the ear loops are made of weak material and break easily.\n",
    "The bottowm line...another worthless, cheap gimmick from Sprint.\n",
    "This pair of headphones is the worst that I have ever had sound-wise.\n",
    "Att is not clear, sound is very distorted and you have to yell when you talk.\n",
    "i would advise to not purchase this item it never worked very well.\n",
    "It doesn't make you look cool.\n",
    "Bought mainly for the charger, which broke soon after purchasing.\n",
    "I put the latest OS on it (v1.15g), and it now likes to slow to a crawl and lock up every once in a while.\n",
    "There's really nothing bad I can say about this headset.\n",
    "We are sending it back.\n",
    "I came over from Verizon because cingulair has nicer cell phones.... the first thing I noticed was the really bad service.\n",
    "Unreliable - I'm giving up.\n",
    "After my phone got to be about a year old, it's been slowly breaking despite much care on my part.\n",
    "It was a waste of my money.\n",
    "don't waste your money and time.\n",
    "Due to this happening on every call I was forced to stop using this headset.\n",
    "I checked everywhere and there is no feature for it which is really disappointing.\n",
    "Does not fit.\n",
    "I'll be looking for a new earpiece.\n",
    "However, the keypads are so tinny that I sometimes reach the wrong buttons.\n",
    "Unfortunately it did not work.\n",
    "Couldn't figure it out\n",
    "Worst software ever used.... If I could give this zero stars I would.\n",
    "Lousy product.\n",
    "Lasted one day and then blew up.\n",
    "I bought it for my mother and she had a problem with the battery.\n",
    "I was not impressed by this product.\n",
    "Not enough volume.\n",
    "Buyer--Be Very Careful!!!!!.\n",
    "Were JERKS on the phone.\n",
    "But when I check voice mail at night, the keypad backlight turns off a few seconds into the first message, and then I'm lost.\n",
    "Disapointing Results.\n",
    "I got the car charger and not even after a week the charger was broken...I went to plug it in and it started smoking.\n",
    "I find this inexcusable and so will probably be returning this phone and perhaps changing carriers.\n",
    "You get what you pay for I guess.\n",
    "Bad Quality.\n",
    "It's not what it says it is.\n",
    "It's kind of embarrassing to use because of how it looks and mostly it's embarrassing how child-like the company is.\n",
    "All in all, I'd expected a better consumer experience from Motorola.\n",
    "Sending it back.\n",
    "Everything about this product is wrong.First\n",
    "It is cheap, and it feel and look just as cheap.\n",
    "After receiving and using the product for just 2 days it broke.\n",
    "Phone falls out easily.\n",
    "The first thing that happened was that the tracking was off.\n",
    "Linksys should have some way to exchange a bad phone for a refurb unit or something!\n",
    "A must study for anyone interested in the \"worst sins\" of industrial design.\n",
    "The BT headset was such a disapoinment.\n",
    "I have had this phone for over a year now, and I will tell you, its not that great.\n",
    "What a piece of junk.. I lose more calls on this phone.\n",
    "Doesn't work at all.. I bougth it for my L7c and its not working.\n",
    "very disappointed.\n",
    "Earbud piece breaks easily.\n",
    "I can barely ever hear on it and am constantly saying \"what?\"\n",
    "It doesn't work in Europe or Asia.\n",
    "I ordered this product first and was unhappy with it immediately.\n",
    "I'm really disappointed all I have now is a charger that doesn't work.\n",
    "Horrible, horrible protector.\n",
    "Rip off---- Over charge shipping.\n",
    "Case was more or less an extra that I originally put on but later discarded because it scratched my ear.\n",
    "I've also had problems with the phone reading the memory card in which I always turn it on and then off again.\n",
    "Piece of Junk.\n",
    "The case is a flimsy piece of plastic and has no front or side protection whatsoever.\n",
    "The battery is completely useless to me.\n",
    "The biggest complaint I have is, the battery drains superfast.\n",
    "So I had to take the battery out of the phone put it all back together and then restart it.\n",
    "Bad Purchase.\n",
    "It was horrible!.\n",
    "Perhaps my phone is defective, but people cannot hear me when I use this.\n",
    "Not only will it drain your player, but may also potentially fry it.\n",
    "Improper description.... I had to return it.\n",
    "Cant get the software to work with my computer.\n",
    "I did not bother contacting the company for few dollar product but I learned the lesson that I should not have bought this form online anyway.\n",
    "Reaching for the bottom row is uncomfortable, and the send and end keys are not where I expect them to be.3.\n",
    "The calls drop, the phone comes on and off at will, the screen goes black and the worst of all it stops ringing intermittently.\n",
    "The commercials are the most misleading.\n",
    "Steer clear of this product and go with the genuine Palm replacementr pens, which come in a three-pack.\n",
    "I don't think it would hold it too securly on your belt.\n",
    "It makes very strange ticking noises before it ends the call.\n",
    "I kept catching the cable on the seat and I had to pull the phone out to turn it on an off.\n",
    "Piece of trash.\n",
    "Not a good item.. It worked for a while then started having problems in my auto reverse tape player.\n",
    "Then a few days later the a puff of smoke came out of the phone while in use.\n",
    "Then I exchanged for the same phone, even that had the same problem.\n",
    "Cumbersome design.\n",
    "All three broke within two months of use.\n",
    "One thing I hate is the mode set button at the side.\n",
    "While I managed to bend the leaf spring back in place, the metal now has enough stress that it will break on the next drop.\n",
    "The camera, although rated at an impressive 1.3 megapixels, renders images that fall well below expectations of such a relatively high resolution.\n",
    "The screen does get smudged easily because it touches your ear and face.\n",
    "Item Does Not Match Picture.\n",
    "DO NOT BUY DO NOT BUYIT SUCKS\n",
    "Doesn't Work.\n",
    "Sprint charges for this service.\n",
    "I am not impressed with this and i would not recommend this item to anyone.\n",
    "This is essentially a communications tool that does not communicate.\n",
    "stay away from this store, be careful.\n",
    "It's A PIECE OF CRAP!\n",
    "sucked, most of the stuff does not work with my phone.\n",
    "Adapter does not provide enough charging current.\n",
    "Talk about USELESS customer service.\n",
    "What a big waste of time.\n",
    "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\n",
    "You also cannot take pictures with it in the case because the lense is covered.\n",
    "Don't make the same mistake that I did and please don't buy this phone.\n",
    "The construction of the headsets is poor.\n",
    "It's uncomfortable and the sound quality is quite poor compared with the phone (Razr) or with my previous wired headset (that plugged into an LG).\n",
    "No additional ear gels provided, and no instructions whatsoever.\n",
    "Echo Problem....Very unsatisfactory\n",
    "These products cover up the important light sensor above the ear outlet.\n",
    "And none of the tones is acceptable.\n",
    "don't waste your money.\n",
    "This is infuriating.\n",
    "The loudspeaker option is great, the bumpers with the lights is very ... appealing.\n",
    "It clicks into place in a way that makes you wonder how long that mechanism would last.\n",
    "I found this product to be waaay too big.\n",
    "The instructions didn't explain that a microphone jack could be used.\n",
    "Uncomfortable In the Ear, Don't use with LG VX9900 (EnV).\n",
    "You have to hold the phone at a particular angle for the other party to hear you clearly.\n",
    "Can't upload ringtones from a third party.\n",
    "Today is the second time I've been to their lunch buffet and it was pretty good.\n",
    "I would recommend saving room for this!\n",
    "This place receives stars for their APPETIZERS!!!\n",
    "It is PERFECT for a sit-down family meal or get together with a few friends.\n",
    "I had heard good things about this place, but it exceeding every hope I could have dreamed of.\n",
    "They were golden-crispy and delicious.\n",
    "The wontons were thin, not thick and chewy, almost melt in your mouth.\n",
    "All in all an excellent restaurant highlighted by great service, a unique menu, and a beautiful setting.\n",
    "Ample portions and good prices.\n",
    "Their regular toasted bread was equally satisfying with the occasional pats of butter... Mmmm...!\n",
    "The food was outstanding and the prices were very reasonable.\n",
    "In an interesting part of town, this place is amazing.\n",
    "I want to first say our server was great and we had perfect service.\n",
    "Im in AZ all the time and now have my new spot.\n",
    "The ambience is wonderful and there is music playing.\n",
    "This is the place where I first had pho and it was amazing!!\n",
    "Both of the egg rolls were fantastic.\n",
    "It was just not a fun experience.\n",
    "Cant say enough good things about this place.\n",
    "The bartender was also nice.\n",
    "Their steaks are 100% recommended!\n",
    "Awesome service and food.\n",
    "Our server was fantastic and when he found out the wife loves roasted garlic and bone marrow, he added extra to our meal and another marrow to go!\n",
    "A good time!\n",
    "I will come back here every time I'm in Vegas.\n",
    "The lighting is just dark enough to set the mood.\n",
    "Service was fantastic.\n",
    "A couple of months later, I returned and had an amazing meal.\n",
    "The food came out at a good pace.\n",
    "Great food and awesome service!\n",
    "I love the decor with the Chinese calligraphy wall paper.\n",
    "Service is perfect and the family atmosphere is nice to see.\n",
    "Once you get inside you'll be impressed with the place.\n",
    "Would come back again if I had a sushi craving while in Vegas.\n",
    "High-quality chicken on the chicken Caesar salad.\n",
    "We were promptly greeted and seated.\n",
    "I tried the Cape Cod ravoli, chicken,with cranberry...mmmm!\n",
    "Fantastic service here.\n",
    "Just had lunch here and had a great experience.\n",
    "The pancake was also really good and pretty large at that.\n",
    "The service was outshining & I definitely recommend the Halibut.\n",
    "I also had to taste my Mom's multi-grain pumpkin pancakes with pecan butter and they were amazing, fluffy, and delicious!\n",
    "A great way to finish a great.\n",
    "I personally love the hummus, pita, baklava, falafels and Baba Ganoush (it's amazing what they do with eggplant!).\n",
    "This place is pretty good, nice little vibe in the restaurant.\n",
    "The chicken was deliciously seasoned and had the perfect fry on the outside and moist chicken on the inside.\n",
    "So flavorful and has just the perfect amount of heat.\n",
    "This wonderful experience made this place a must-stop whenever we are in town again.\n",
    "We were sat right on time and our server from the get go was FANTASTIC!\n",
    "OMG I felt like I had never eaten Thai food until this dish.\n",
    "The sergeant pepper beef sandwich with auju sauce is an excellent sandwich as well.\n",
    "Very very fun chef.\n",
    "The food, amazing.\n",
    "An excellent new restaurant by an experienced Frenchman.\n",
    "I will be back many times soon.\n",
    "There was a warm feeling with the service and I felt like their guest for a special treat.\n",
    "The atmosphere here is fun.\n",
    "I'm so happy to be here!!!\"\n",
    "Their chow mein is so good!\n",
    "The service was great, even the manager came and helped with our table.\n",
    "Just as good as when I had it more than a year ago!\n",
    "What I really like there is the crepe station.\n",
    "It was absolutely amazing.\n",
    "Some may say this buffet is pricey but I think you get what you pay for and this place you are getting quite a lot!\n",
    "On the up side, their cafe serves really good food.\n",
    "We thought you'd have to venture further away to get good sushi, but this place really hit the spot that night.\n",
    "Everyone is treated equally special.\n",
    "This isn't a small family restaurant, this is a fine dining establishment.\n",
    "I'll definitely be in soon again.\n",
    "I miss it and wish they had one in Philadelphia!\n",
    "This is an Outstanding little restaurant with some of the Best Food I have ever tasted.\n",
    "Great place to have a couple drinks and watch any and all sporting events as the walls are covered with TV's.\n",
    "Pretty cool I would say.\n",
    "The flair bartenders are absolutely amazing!\n",
    "The croutons also taste homemade which is an extra plus.\n",
    "the potatoes were great and so was the biscuit.\n",
    "So they performed.\n",
    "It's worth driving up from Tucson!\n",
    "Best tacos in town by far!!\n",
    "The chips and salsa were really good, the salsa was very fresh.\n",
    "Our server was super nice and checked on us many times.\n",
    "The cocktails are all handmade and delicious.\n",
    "They know how to make them here.\n",
    "We made the drive all the way from North Scottsdale... and I was not one bit disappointed!\n",
    "Great service and food.\n",
    "We loved the biscuits!!!\n",
    "Definitely worth venturing off the strip for the pork belly, will return next time I'm in Vegas.\n",
    "They really want to make your experience a good one.\n",
    "Favorite place in town for shawarrrrrrma!!!!!!\n",
    "The burger is good beef, cooked just right.\n",
    "The seasonal fruit was fresh white peach puree.\n",
    "Great place to eat, reminds me of the little mom and pop shops in the San Francisco Bay Area.\n",
    "Hawaiian Breeze, Mango Magic, and Pineapple Delight are the smoothies that I've tried so far and they're all good.\n",
    "The food is good.\n",
    "The atmosphere is modern and hip, while maintaining a touch of coziness.\n",
    "Best tater tots in the southwest.\n",
    "Back to good BBQ, lighter fare, reasonable pricing and tell the public they are back to the old ways.\n",
    "Penne vodka excellent!\n",
    "I believe that this place is a great stop for those with a huge belly and hankering for sushi.\n",
    "Cute, quaint, simple, honest.\n",
    "A fantastic neighborhood gem !!!\n",
    "Now the pizza itself was good the peanut sauce was very tasty.\n",
    "It was awesome.\n",
    "The nachos are a MUST HAVE!\n",
    "I like Steiners because it's dark and it feels like a bar.\n",
    "I had the opportunity today to sample your amazing pizzas!\n",
    "At first glance it is a lovely bakery cafe - nice ambiance, clean, friendly staff.\n",
    "Waitress was good though!\n",
    "You get incredibly fresh fish, prepared with care.\n",
    "These were so good we ordered them twice.\n",
    "Service is quick and friendly.\n",
    "I liked the patio and the service was outstanding.\n",
    "All of the tapas dishes were delicious!\n",
    "In the summer, you can dine in a charming outdoor patio - so very delightful.\n",
    "They have a good selection of food including a massive meatloaf sandwich, a crispy chicken wrap, a delish tuna melt and some tasty burgers.\n",
    "The food was excellent and service was very good.\n",
    "I have been here several times in the past, and the experience has always been great.\n",
    "Great place fo take out or eat in.\n",
    "An absolute must visit!\n",
    "Very friendly staff.\n",
    "I loved the bacon wrapped dates.\n",
    "Last night was my second time dining here and I was so happy I decided to go back!\n",
    "They have a plethora of salads and sandwiches, and everything I've tried gets my seal of approval.\n",
    "The selection on the menu was great and so were the prices.\n",
    "All in all, I can assure you I'll be back.\n",
    "Great food.\n",
    "Food was good, service was good, Prices were good.\n",
    "The black eyed peas and sweet potatoes... UNREAL!\n",
    "As always the evening was wonderful and the food delicious!\n",
    "I have eaten here multiple times, and each time the food was delicious.\n",
    "My fianc√© and I came in the middle of the day and we were greeted and seated right away.\n",
    "And considering the two of us left there very full and happy for about $20, you just can't go wrong.\n",
    "Great food and service, huge portions and they give a military discount.\n",
    "I *heart* this place.\n",
    "this place is good.\n",
    "I love the owner/chef, his one authentic Japanese cool dude!\n",
    "I didn't know pulled pork could be soooo delicious.\n",
    "I recently tried Caballero's and I have been back every week since!\n",
    "The food was very good and I enjoyed every mouthful, an enjoyable relaxed venue for couples small family groups etc.\n",
    "CONCLUSION: Very filling meals.\n",
    "You won't be disappointed.\n",
    "I was proven dead wrong by this sushi bar, not only because the quality is great, but the service is fast and the food, impeccable.\n",
    "The steak and the shrimp are in my opinion the best entrees at GC.\n",
    "Restaurant is always full but never a wait.\n",
    "Love this place, hits the spot when I want something healthy but not lacking in quantity or flavor.\n",
    "The fries were great too.\n",
    "I don't each much pasta, but I love the homemade /hand made pastas and thin pizzas here.\n",
    "This was my first crawfish experience, and it was delicious!\n",
    "On a positive note, our server was very attentive and provided great service.\n",
    "I had strawberry tea, which was good.\n",
    "The waitress was friendly and happy to accomodate for vegan/veggie options.\n",
    "We loved the place.\n",
    "Nice, spicy and tender.\n",
    "We walked away stuffed and happy about our first Vegas buffet experience.\n",
    "Point your finger at any item on the menu, order it and you won't be disappointed.\n",
    "The sides are delish - mixed mushrooms, yukon gold puree, white corn - beateous.\n",
    "Service was fine and the waitress was friendly.\n",
    "Generous portions and great taste.\n",
    "The roast beef sandwich tasted really good!\n",
    "Loved it...friendly servers, great food, wonderful and imaginative menu.\n",
    "I had a seriously solid breakfast here.\n",
    "We had a group of 70+ when we claimed we would only have 40 and they handled us beautifully.\n",
    "Omelets are to die for!\n",
    "They had a toro tartare with a cavier that was extraordinary and I liked the thinly sliced wagyu with white truffle.\n",
    "The grilled chicken was so tender and yellow from the saffron seasoning.\n",
    "To summarize... the food was incredible, nay, transcendant... but nothing brings me joy quite like the memory of the pneumatic condiment dispenser.\n",
    "Great food for the price, which is very high quality and house made.\n",
    "Best Buffet in town, for the price you cannot beat it.\n",
    "Anyway, this FS restaurant has a wonderful breakfast/lunch.\n",
    "The ambiance was incredible.\n",
    "The atmosphere was great with a lovely duo of violinists playing songs we requested.\n",
    "This place is amazing!\n",
    "The food was very good.\n",
    "The portion was huge!\n",
    "The goat taco didn't skimp on the meat and wow what FLAVOR!\n",
    "The food was great as always, compliments to the chef.\n",
    "* Both the Hot & Sour & the Egg Flower Soups were absolutely 5 Stars!\n",
    "He deserves 5 stars.\n",
    "Lordy, the Khao Soi is a dish that is not to be missed for curry lovers!\n",
    "This place is awesome if you want something light and healthy during the summer.\n",
    "I was seated immediately.\n",
    "I will continue to come here on ladies night andddd date night ... highly recommend this place to anyone who is in the area (;\n",
    "They also have the best cheese crisp in town.\n",
    "Food was delicious!\n",
    "Both of them were truly unbelievably good, and I am so glad we went back.\n",
    "you can watch them preparing the delicious food!)\n",
    "The cow tongue and cheek tacos are amazing.\n",
    "This was my first time and I can't wait until the next.\n",
    "The staff are also very friendly and efficient.\n",
    "Of all the dishes, the salmon was the best, but all were great.\n",
    "Food was so gooodd.\n",
    "They have great dinners.\n",
    "Best fish I've ever had in my life!\n",
    "Great brunch spot.\n",
    "I don't have very many words to say about this place, but it does everything pretty well.\n",
    "The sweet potato fries were very good and seasoned well.\n",
    "I could eat their bruschetta all day it is devine.\n",
    "Ambience is perfect.\n",
    "We ordered the duck rare and it was pink and tender on the inside with a nice char on the outside.\n",
    "Service was good and the company was better!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cleaning text into a list of tokens\n",
    "\n",
    "Our goal here is to represent each line of text in the \"raw\" data as a list of tokens.\n",
    "\n",
    "A 'token' here is just a string of non-whitespace characters.\n",
    "\n",
    "We'll make some simple decisions:\n",
    "\n",
    "* Make everything lower case\n",
    "* Remove any punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(raw_text):\n",
    "    ''' Transform a plain-text string into a list of tokens\n",
    "    \n",
    "    We assume that *whitespace* divides tokens.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    raw_text : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list_of_tokens : list of strings\n",
    "        Each element is one token in the provided text\n",
    "    '''\n",
    "    list_of_tokens = raw_text.split() # split method divides on whitespace by default\n",
    "    for pp in range(len(list_of_tokens)):\n",
    "        cur_token = list_of_tokens[pp]\n",
    "        # Remove punctuation\n",
    "        for punc in ['?', '!', '_', '.', ',', '\"', '/']:\n",
    "            cur_token = cur_token.replace(punc, \"\")\n",
    "        # Turn to lower case\n",
    "        clean_token = cur_token.lower()\n",
    "        # Replace the cleaned token into the original list\n",
    "        list_of_tokens[pp] = clean_token\n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of turning the text into a list of tokens\n",
    "\n",
    "Lets show the raw and token-list representations of the first 10 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "Oh and I forgot to also mention the weird color effect it has on your phone.\n",
      "Clean token list:\n",
      "['oh', 'and', 'i', 'forgot', 'to', 'also', 'mention', 'the', 'weird', 'color', 'effect', 'it', 'has', 'on', 'your', 'phone']\n",
      "Raw text:\n",
      "THAT one didn't work either.\n",
      "Clean token list:\n",
      "['that', 'one', \"didn't\", 'work', 'either']\n",
      "Raw text:\n",
      "Waste of 13 bucks.\n",
      "Clean token list:\n",
      "['waste', 'of', '13', 'bucks']\n",
      "Raw text:\n",
      "Product is useless, since it does not have enough charging current to charge the 2 cellphones I was planning to use it with.\n",
      "Clean token list:\n",
      "['product', 'is', 'useless', 'since', 'it', 'does', 'not', 'have', 'enough', 'charging', 'current', 'to', 'charge', 'the', '2', 'cellphones', 'i', 'was', 'planning', 'to', 'use', 'it', 'with']\n",
      "Raw text:\n",
      "None of the three sizes they sent with the headset would stay in my ears.\n",
      "Clean token list:\n",
      "['none', 'of', 'the', 'three', 'sizes', 'they', 'sent', 'with', 'the', 'headset', 'would', 'stay', 'in', 'my', 'ears']\n",
      "Raw text:\n",
      "Worst customer service.\n",
      "Clean token list:\n",
      "['worst', 'customer', 'service']\n",
      "Raw text:\n",
      "The Ngage is still lacking in earbuds.\n",
      "Clean token list:\n",
      "['the', 'ngage', 'is', 'still', 'lacking', 'in', 'earbuds']\n",
      "Raw text:\n",
      "It always cuts out and makes a beep beep beep sound then says signal failed.\n",
      "Clean token list:\n",
      "['it', 'always', 'cuts', 'out', 'and', 'makes', 'a', 'beep', 'beep', 'beep', 'sound', 'then', 'says', 'signal', 'failed']\n",
      "Raw text:\n",
      "the only VERY DISAPPOINTING thing was there was NO SPEAKERPHONE!!!!\n",
      "Clean token list:\n",
      "['the', 'only', 'very', 'disappointing', 'thing', 'was', 'there', 'was', 'no', 'speakerphone']\n",
      "Raw text:\n",
      "Very disappointed in AccessoryOne.\n",
      "Clean token list:\n",
      "['very', 'disappointed', 'in', 'accessoryone']\n"
     ]
    }
   ],
   "source": [
    "for line in all_reviews_as_line_separated_string.split(\"\\n\")[:10]:\n",
    "    print(\"Raw text:\")\n",
    "    print(line)\n",
    "    print(\"Clean token list:\")\n",
    "    print(tokenize_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a fixed-size vocabulary\n",
    "\n",
    "We want to select candidate tokens for our vocabulary.\n",
    "\n",
    "Let's use the following *simple* rules to build our vocabulary\n",
    "\n",
    "* Keep any token that appears at least 4 times in our corpus (entire dataset)\n",
    "\n",
    "Why? If a token is *rare*, it might be difficult to learn a reliable pattern for how it can be used to predict sentiment, which is our ultimate goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_count_dict = dict()\n",
    "\n",
    "for line in all_reviews_as_line_separated_string.split(\"\\n\"):\n",
    "    tok_list = tokenize_text(line)\n",
    "    for tok in tok_list:\n",
    "        if tok in tok_count_dict:\n",
    "            tok_count_dict[tok] += 1\n",
    "        else:\n",
    "            tok_count_dict[tok] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 10 most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = list(sorted(tok_count_dict, key=tok_count_dict.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  255 the\n",
      "  150 and\n",
      "  122 i\n",
      "   91 a\n",
      "   83 was\n",
      "   81 it\n",
      "   79 is\n",
      "   72 to\n",
      "   63 this\n",
      "   53 in\n"
     ]
    }
   ],
   "source": [
    "for w in sorted_tokens[:10]:\n",
    "    print(\"%5d %s\" % (tok_count_dict[w], w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 10 least common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1 life\n",
      "    1 brunch\n",
      "    1 words\n",
      "    1 potato\n",
      "    1 bruschetta\n",
      "    1 devine\n",
      "    1 duck\n",
      "    1 rare\n",
      "    1 pink\n",
      "    1 char\n"
     ]
    }
   ],
   "source": [
    "for w in sorted_tokens[-10:]:\n",
    "    print(\"%5d %s\" % (tok_count_dict[w], w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary as list of all tokens that have count at least 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings that identify all tokens in our vocabulary\n",
    "# We'll use a *list comprehension*, a way in Python to cleaning select a subset of a larger list\n",
    "# by providing an if statement.\n",
    "\n",
    "vocab_list = [w for w in sorted_tokens if tok_count_dict[w] >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  255 the\n",
      "  150 and\n",
      "  122 i\n",
      "   91 a\n",
      "   83 was\n",
      "   81 it\n",
      "   79 is\n",
      "   72 to\n",
      "   63 this\n",
      "   53 in\n",
      "   49 of\n",
      "   41 not\n",
      "   39 for\n",
      "   39 good\n",
      "   36 with\n",
      "   33 on\n",
      "   33 my\n",
      "   31 very\n",
      "   31 great\n",
      "   30 that\n",
      "   30 had\n",
      "   29 phone\n",
      "   27 have\n",
      "   27 service\n",
      "   26 you\n",
      "   26 food\n",
      "   25 place\n",
      "   23 are\n",
      "   22 but\n",
      "   22 all\n",
      "   21 so\n",
      "   20 were\n",
      "   19 we\n",
      "   18 they\n",
      "   18 be\n",
      "   15 time\n",
      "   14 at\n",
      "   14 an\n",
      "   13 also\n",
      "   13 would\n",
      "   13 here\n",
      "   13 back\n",
      "   12 work\n",
      "   12 first\n",
      "   12 really\n",
      "   11 product\n",
      "   11 ear\n",
      "   11 amazing\n",
      "   10 has\n",
      "   10 your\n",
      "   10 headset\n",
      "   10 out\n",
      "   10 what\n",
      "   10 just\n",
      "   10 from\n",
      "   10 about\n",
      "   10 get\n",
      "   10 delicious\n",
      "    9 does\n",
      "    9 use\n",
      "    9 then\n",
      "    9 don't\n",
      "    9 i'm\n",
      "    9 like\n",
      "    9 it's\n",
      "    9 will\n",
      "    8 one\n",
      "    8 disappointed\n",
      "    8 me\n",
      "    8 off\n",
      "    8 could\n",
      "    8 because\n",
      "    8 battery\n",
      "    8 as\n",
      "    8 when\n",
      "    8 experience\n",
      "    8 or\n",
      "    8 their\n",
      "    8 our\n",
      "    8 best\n",
      "    7 enough\n",
      "    7 there\n",
      "    7 no\n",
      "    7 bad\n",
      "    7 quality\n",
      "    7 doesn't\n",
      "    7 made\n",
      "    7 item\n",
      "    7 piece\n",
      "    7 if\n",
      "    7 i've\n",
      "    7 now\n",
      "    7 every\n",
      "    7 which\n",
      "    7 restaurant\n",
      "    7 nice\n",
      "    6 waste\n",
      "    6 2\n",
      "    6 always\n",
      "    6 sound\n",
      "    6 only\n",
      "    6 horrible\n",
      "    6 them\n",
      "    6 make\n",
      "    6 tried\n",
      "    6 recommend\n",
      "    6 pretty\n",
      "    6 been\n",
      "    6 excellent\n",
      "    6 way\n",
      "    6 new\n",
      "    6 go\n",
      "    6 by\n",
      "    6 well\n",
      "    6 ever\n",
      "    6 up\n",
      "    6 while\n",
      "    6 say\n",
      "    6 perfect\n",
      "    6 town\n",
      "    6 fantastic\n",
      "    6 chicken\n",
      "    6 friendly\n",
      "    5 didn't\n",
      "    5 worst\n",
      "    5 thing\n",
      "    5 times\n",
      "    5 do\n",
      "    5 these\n",
      "    5 junk\n",
      "    5 into\n",
      "    5 after\n",
      "    5 poor\n",
      "    5 some\n",
      "    5 charger\n",
      "    5 can\n",
      "    5 came\n",
      "    5 -\n",
      "    5 night\n",
      "    5 server\n",
      "    5 wonderful\n",
      "    5 love\n",
      "    5 happy\n",
      "    4 charge\n",
      "    4 still\n",
      "    4 same\n",
      "    4 did\n",
      "    4 bought\n",
      "    4 both\n",
      "    4 put\n",
      "    4 can't\n",
      "    4 anyone\n",
      "    4 impressed\n",
      "    4 even\n",
      "    4 went\n",
      "    4 its\n",
      "    4 \n",
      "    4 little\n",
      "    4 plug\n",
      "    4 know\n",
      "    4 call\n",
      "    4 year\n",
      "    4 right\n",
      "    4 too\n",
      "    4 many\n",
      "    4 design\n",
      "    4 case\n",
      "    4 huge\n",
      "    4 wrong\n",
      "    4 take\n",
      "    4 never\n",
      "    4 us\n",
      "    4 last\n",
      "    4 over\n",
      "    4 easily\n",
      "    4 stars\n",
      "    4 few\n",
      "    4 how\n",
      "    4 more\n",
      "    4 again\n",
      "    4 come\n",
      "    4 away\n",
      "    4 buffet\n",
      "    4 family\n",
      "    4 menu\n",
      "    4 prices\n",
      "    4 want\n",
      "    4 spot\n",
      "    4 awesome\n",
      "    4 vegas\n",
      "    4 atmosphere\n",
      "    4 sushi\n",
      "    4 sandwich\n",
      "    4 loved\n"
     ]
    }
   ],
   "source": [
    "for w in vocab_list:\n",
    "    print(\"%5d %s\" % (tok_count_dict[w], w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2a: Do you see tokens in the vocabulary that might be useful for the sentiment prediction? What are they? Are there some that would never be useful? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO make a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b: How many tokens are in the chosen vocabulary? How many would there be if you used count at least 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating bag-of-words representation for individual review\n",
    "\n",
    "Now, given the vocabulary we defined above in part 2, let's turn each text into a count vector.\n",
    "\n",
    "Our goal is to write a method that can take each individual review text and produce a feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a dictionary that maps each vocab term in order to an integer defining its order in the vocab\n",
    "\n",
    "Each vocab term maps to a unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = dict()\n",
    "for vocab_id, tok in enumerate(vocab_list):\n",
    "    vocab_dict[tok] = vocab_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to produce feature vector from provided text and the vocabulary (as a dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_into_feature_vector(text, vocab_dict):\n",
    "    ''' Produce count feature vector for provided text\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    text : string\n",
    "        A string of raw text, representing a single 'review'\n",
    "    vocab_dict : dict with string keys\n",
    "        If token is in vocabulary, will exist as key in the dict\n",
    "        If token is not in vocabulary, will not be in the dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count_V : 1D numpy array, shape (V,) = (n_vocab,)\n",
    "        Count vector, indicating how often each vocab word\n",
    "        appears in the provided text string\n",
    "    '''\n",
    "    V = len(vocab_dict.keys())\n",
    "    count_V = np.zeros(V)\n",
    "    for tok in tokenize_text(text):\n",
    "        if tok in vocab_dict:\n",
    "            vv = vocab_dict[tok]\n",
    "            count_V[vv] += 1\n",
    "    return count_V\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example tranformations of short phrases\n",
    "\n",
    "Below, we'll try our count-vector transformation on several manually constructed short 'documents'\n",
    "\n",
    "* Common words: \"the was this\"\n",
    "* Positive words: \"good great fantastic\"\n",
    "* Negative words: \"bad horrible awful\"\n",
    "* Nonsense words: \"dinosaur nonsense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 2., 1., 0., 0., 0., 1., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common words (should produce many positive entries in first few positions of the vector)\n",
    "transform_text_into_feature_vector(\"the was this the of a an of a\", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive words (should produce a few positive entries!)\n",
    "transform_text_into_feature_vector(\"good great fantastic excellent good\", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative words (should produce a few positive entries!)\n",
    "transform_text_into_feature_vector(\"bad worse awful terrible horrible\", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rare / nonsense words (should produce an all-zero vector!)\n",
    "transform_text_into_feature_vector(\"dinosaur nonsense supercalifragilisticexpealidocious\", vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example tranformations of actual review (first row, index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh and I forgot to also mention the weird color effect it has on your phone.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text_0 = all_reviews_as_line_separated_string.split(\"\\n\")[0]\n",
    "print(raw_text_0)\n",
    "transform_text_into_feature_vector(raw_text_0, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example tranformations of actual review (index 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't figure it out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text_101 = all_reviews_as_line_separated_string.split(\"\\n\")[101]\n",
    "print(raw_text_101)\n",
    "transform_text_into_feature_vector(raw_text_101, vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Using a bag-of-words representation for a classifier\n",
    "\n",
    "Let's show how we can classify text reviews using our BoW count feature representation.\n",
    "\n",
    "We'll assume we have N total reviews in our training set.\n",
    "\n",
    "We have defined a vocabulary of V terms (in part 2 above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(all_reviews_as_line_separated_string.split(\"\\n\"))\n",
    "V = len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the labels $y$ for all reviews\n",
    "\n",
    "We'll use knowledge that we built the raw dataset here by stacking many negative reviews then many positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_N = np.hstack([np.zeros(N//2), np.ones(N//2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to bow features $x$ for all reviews\n",
    "\n",
    "We need a feature matrix $X$ (with N rows and V features). We can do this just stacking all the transformed features from individual reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_NV = np.zeros((N, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nn, raw_text_line in enumerate(all_reviews_as_line_separated_string.split(\"\\n\")):\n",
    "    x_tr_NV[nn] = transform_text_into_feature_vector(raw_text_line, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 194)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_NV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4a: How many times does each word in vocabulary appear in our training set?\n",
    "\n",
    "Hint: use np.sum with a specific axis specified, for the x_tr_NV array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a binary classifier\n",
    "\n",
    "Let's train a LogisticRegression classifier.\n",
    "\n",
    "We'll do 20 iters to keep it fast.\n",
    "\n",
    "Don't worry if you get warnings about convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(C=1000.0, max_iter=20) # Just pick reasonable choices for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, max_iter=20)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_tr_NV, y_tr_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is accuracy of our classifier on training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "yhat_tr_N = clf.predict(x_tr_NV)\n",
    "acc = np.mean( y_tr_N == yhat_tr_N )\n",
    "\n",
    "print(\"Training accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the learned logistic regression weights for each token in our vocabulary?\n",
    "\n",
    "Each token in our vocabulary is a feature in our model.\n",
    "\n",
    "Logistic regression will learn one weight parameter for each feature (each vocab token)\n",
    "\n",
    "In the code below, we look at the learned weights, sort them from decreasing to increasing order, and then print them next to the corresponding feature. This might help us *interpret* what has been learned.\n",
    "\n",
    "Interpretation: \n",
    "* If a weight is very negative, the more that word appears, the more likely that review is a NEGATIVE sentiment one\n",
    "* If a weight is very positive, the more that word appears, the more likley that review is a POSITIVE sentiment one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.011 phone\n",
      "-14.944 2\n",
      "-13.671 headset\n",
      "-11.589 no\n",
      "-11.479 sound\n",
      "-10.975 battery\n",
      "-10.796 bad\n",
      "-10.516 new\n",
      "-10.466 waste\n",
      "-10.388 still\n",
      " -9.777 product\n",
      " -9.750 case\n",
      " -9.597 thing\n",
      " -9.514 it\n",
      " -9.203 horrible\n",
      " -8.713 use\n",
      " -8.597 then\n",
      " -8.420 plug\n",
      " -8.029 quality\n",
      " -7.619 work\n",
      " -7.434 poor\n",
      " -7.391 piece\n",
      " -7.307 them\n",
      " -7.278 \n",
      " -7.066 but\n",
      " -6.818 of\n",
      " -6.689 ear\n",
      " -6.664 worst\n",
      " -6.388 i'm\n",
      " -6.309 that\n",
      " -6.126 doesn't\n",
      " -5.594 junk\n",
      " -5.074 don't\n",
      " -5.071 design\n",
      " -4.760 if\n",
      " -4.759 has\n",
      " -4.623 charger\n",
      " -4.513 wrong\n",
      " -4.490 from\n",
      " -4.402 not\n",
      " -4.130 do\n",
      " -4.088 this\n",
      " -4.082 charge\n",
      " -4.076 into\n",
      " -4.034 too\n",
      " -3.917 me\n",
      " -3.913 off\n",
      " -3.808 its\n",
      " -3.784 what\n",
      " -3.701 service\n",
      " -3.548 after\n",
      " -3.445 over\n",
      " -3.402 anyone\n",
      " -3.194 easily\n",
      " -3.065 only\n",
      " -2.874 same\n",
      " -2.857 take\n",
      " -2.744 did\n",
      " -2.723 call\n",
      " -2.614 bought\n",
      " -2.554 huge\n",
      " -2.383 could\n",
      " -2.380 both\n",
      " -2.362 enough\n",
      " -2.180 when\n",
      " -2.172 last\n",
      " -2.152 put\n",
      " -2.129 out\n",
      " -2.095 does\n",
      " -1.983 in\n",
      " -1.862 more\n",
      " -1.819 disappointed\n",
      " -1.739 these\n",
      " -1.733 right\n",
      " -1.677 my\n",
      " -1.532 times\n",
      " -1.389 now\n",
      " -1.337 impressed\n",
      " -1.319 ever\n",
      " -1.244 as\n",
      " -1.132 away\n",
      " -0.999 i\n",
      " -0.998 went\n",
      " -0.993 even\n",
      " -0.891 about\n",
      " -0.889 year\n",
      " -0.765 there\n",
      " -0.626 make\n",
      " -0.581 can't\n",
      " -0.560 back\n",
      " -0.513 which\n",
      " -0.351 the\n",
      " -0.329 you\n",
      " -0.285 how\n",
      " -0.141 know\n",
      " -0.041 go\n",
      " -0.025 item\n",
      "  0.030 are\n",
      "  0.140 with\n",
      "  0.172 while\n",
      "  0.273 one\n",
      "  0.362 at\n",
      "  0.515 really\n",
      "  0.692 well\n",
      "  0.720 came\n",
      "  0.737 up\n",
      "  0.792 because\n",
      "  0.847 a\n",
      "  0.919 few\n",
      "  1.000 all\n",
      "  1.122 us\n",
      "  1.143 way\n",
      "  1.220 some\n",
      "  1.286 come\n",
      "  1.292 didn't\n",
      "  1.417 very\n",
      "  1.589 also\n",
      "  1.600 for\n",
      "  1.646 is\n",
      "  1.778 been\n",
      "  1.787 little\n",
      "  1.888 night\n",
      "  1.905 every\n",
      "  1.955 prices\n",
      "  2.003 never\n",
      "  2.065 great\n",
      "  2.106 would\n",
      "  2.114 to\n",
      "  2.283 experience\n",
      "  2.325 many\n",
      "  2.429 server\n",
      "  2.446 your\n",
      "  2.490 will\n",
      "  2.523 always\n",
      "  2.789 or\n",
      "  3.112 -\n",
      "  3.220 by\n",
      "  3.259 want\n",
      "  3.275 sandwich\n",
      "  3.405 first\n",
      "  3.506 we\n",
      "  3.597 can\n",
      "  3.632 get\n",
      "  3.772 on\n",
      "  3.796 made\n",
      "  3.893 so\n",
      "  4.013 our\n",
      "  4.110 and\n",
      "  4.226 again\n",
      "  4.644 happy\n",
      "  4.845 buffet\n",
      "  5.188 have\n",
      "  5.381 sushi\n",
      "  5.516 i've\n",
      "  5.872 had\n",
      "  6.492 recommend\n",
      "  6.510 wonderful\n",
      "  6.659 family\n",
      "  6.672 stars\n",
      "  6.686 excellent\n",
      "  6.787 atmosphere\n",
      "  6.822 town\n",
      "  6.906 be\n",
      "  6.921 tried\n",
      "  6.999 spot\n",
      "  7.025 time\n",
      "  7.121 pretty\n",
      "  7.153 say\n",
      "  7.216 an\n",
      "  7.240 menu\n",
      "  7.795 good\n",
      "  7.810 loved\n",
      "  7.968 chicken\n",
      "  8.324 awesome\n",
      "  8.567 like\n",
      "  8.878 vegas\n",
      "  8.924 nice\n",
      "  9.454 perfect\n",
      "  9.473 just\n",
      "  9.550 it's\n",
      "  9.969 they\n",
      "  9.989 delicious\n",
      " 10.082 was\n",
      " 10.369 restaurant\n",
      " 10.399 love\n",
      " 10.468 here\n",
      " 10.835 friendly\n",
      " 11.716 fantastic\n",
      " 12.515 best\n",
      " 12.709 amazing\n",
      " 13.987 were\n",
      " 15.343 their\n",
      " 17.644 food\n",
      " 18.232 place\n"
     ]
    }
   ],
   "source": [
    "weights_V = clf.coef_[0]\n",
    "sorted_tok_ids_V = np.argsort(weights_V)\n",
    "\n",
    "for vv in sorted_tok_ids_V:\n",
    "    print(\"% 7.3f %s\" % (weights_V[vv], vocab_list[vv]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4b: Can you interpret these weights? What vocab terms would you expect to have large negative or large positive weights? Do they? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO discuss and make a list of what makes sense and what might not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4c: Try out your classifier on new data\n",
    "\n",
    "Below are the raw text of 20 possible reviews, which are NOT in the original dataset we used in Parts 1-3.\n",
    "\n",
    "What does your classifier predict for each one? Does your classifier generalize well to this new data?\n",
    "\n",
    "(You can use your human ability to read to decide what the labels should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = \"\"\"Do not make the same mistake as me.\n",
    "I might have gotten a defect, but I would not risk buying it again because of the built quality alone.\n",
    "Not worth it.\n",
    "you could only take 2 videos at a time and the quality was very poor.\n",
    "If you plan to use this in a car forget about it.\n",
    "I have 2-3 bars on my cell phone when I am home, but you cant not hear anything.\n",
    "Battery has no life.\n",
    "The internet access was fine, it the rare instance that it worked.\n",
    "Saggy, floppy piece of junk.\n",
    "wont work right or atleast for me.\n",
    "Good service, very clean, and inexpensive, to boot!\n",
    "The owners are super friendly and the staff is courteous.\n",
    "Very good food, great atmosphere.1\n",
    "This was my first and only Vegas buffet and it did not disappoint.\n",
    "Interesting decor.\n",
    "Plus, it's only 8 bucks.\n",
    "Great steak, great sides, great wine, amazing desserts.\n",
    "Four stars for the food & the guy in the blue shirt for his great vibe & still letting us in to eat !\n",
    "The staff is great, the food is delish, and they have an incredible beer selection.\n",
    "Nargile - I think you are great\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a prediction for the first example test review\n",
    "x_V = transform_text_into_feature_vector(\"Not worth it.\", vocab_dict)\n",
    "clf.predict(x_V.reshape((1,V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make predictions for the 20 test reviews above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Using built-in sklearn tool: CountVectorizer\n",
    "\n",
    "Instead of writing manual code, you can use CountVectorizer class from sklearn.\n",
    "\n",
    "This class lets you do fast and repeatable BoW feature extraction given a dataset.\n",
    "\n",
    "You can control things like:\n",
    "\n",
    "* How to ignore rare vocab terms.\n",
    "* How to ignore too common vocab terms (like the, an, a that don't provide meaningful semantic content)\n",
    "* How to ignore punctuation\n",
    "* Whether to use single-word features, or ordered pairs (bigrams aka 2-grams) or even 3-grams.\n",
    "* Whether to produce count or binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the docs for CountVectorizer\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "### Parameters of the CountVectorizer\n",
    "\n",
    "Here are all the settings you can control when you construct it.\n",
    "\n",
    "```\n",
    "Parameters\n",
    "----------\n",
    "input : string {'filename', 'file', 'content'}, default='content'\n",
    "    If 'filename', the sequence passed as an argument to fit is\n",
    "    expected to be a list of filenames that need reading to fetch\n",
    "    the raw content to analyze.\n",
    "\n",
    "    If 'file', the sequence items must have a 'read' method (file-like\n",
    "    object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    Otherwise the input is expected to be a sequence of items that\n",
    "    can be of type string or byte.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : string {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer is not callable``.\n",
    "\n",
    "analyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "```\n",
    "\n",
    "### Attributes of the CountVectorizer\n",
    "\n",
    "After you fit the count vectorizer, you can access these attributes.\n",
    "\n",
    "```\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_: boolean\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our training data (make it an iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_training_text_reviews = all_reviews_as_line_separated_string.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CountVectorizer that uses our *predefined* vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_preprocessor = CountVectorizer(binary=False, vocabulary=vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(vocabulary={'': 156, '-': 137, '2': 97, 'a': 3, 'about': 55,\n",
       "                            'after': 131, 'again': 179, 'all': 29, 'also': 38,\n",
       "                            'always': 98, 'amazing': 47, 'an': 37, 'and': 1,\n",
       "                            'anyone': 151, 'are': 27, 'as': 73, 'at': 36,\n",
       "                            'atmosphere': 190, 'away': 181, 'awesome': 188,\n",
       "                            'back': 41, 'bad': 83, 'battery': 72, 'be': 34,\n",
       "                            'because': 71, 'been': 107, 'best': 79, 'both': 148,\n",
       "                            'bought': 147, 'buffet': 182, ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_preprocessor.fit(list_of_training_text_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we get the features?\n",
    "\n",
    "Use sklearn API's `transform` method of your trained extractor.\n",
    "\n",
    "This will deliver a SPARSE matrix representation (since so many entries are exactly zero, as most documents do not contain any instances of many vocab terms).\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package.\n",
    "\n",
    "See scipy.sparse docs for details: <https://docs.scipy.org/doc/scipy/reference/sparse.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3, 194)\n"
     ]
    }
   ],
   "source": [
    "sparse_arr = bow_preprocessor.transform(list_of_training_text_reviews[:3])\n",
    "print(type(sparse_arr))\n",
    "print(sparse_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert to a dense representaiton via the `toarray()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3, 194)\n"
     ]
    }
   ],
   "source": [
    "dense_arr_3V = sparse_arr.toarray()\n",
    "print(type(dense_arr_3V))\n",
    "print(dense_arr_3V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the extracted count features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_arr_3V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer that builds its own UNIGRAM vocabulary\n",
    "\n",
    "* ngram_range=(1,1) : Means it will use unigrams (individual tokens)\n",
    "* min_df=1 : Means include any term that occurs in at least one document in training set\n",
    "* max_df=1.0 : Means include any terms that appears in less than 100% (fraction of 1.0) of training docs\n",
    "* binary=False : means it will do counts, not just binary presence/absence of each vocab term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_preprocessor = CountVectorizer(ngram_range=(1,1), min_df=1, max_df=1.0, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_preprocessor.fit(list_of_training_text_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_preprocessor.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 714 oh\n",
      "  40 and\n",
      " 421 forgot\n",
      "1071 to\n",
      "  30 also\n",
      " 646 mention\n",
      "1045 the\n",
      "1166 weird\n",
      " 209 color\n",
      " 338 effect\n"
     ]
    }
   ],
   "source": [
    "for term, count in list(bow_preprocessor.vocabulary_.items())[:10]:\n",
    "    print(\"%4d %s\" % (count, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer that builds its own vocabulary with 2-grams \n",
    "\n",
    "2-grams means we look at an ordered pair of words, not individual words.\n",
    "\n",
    "So \"New York\", \"very bad\", \"not good\", and \"the cat\" are all 2-grams (aka bigrams).\n",
    "\n",
    "\n",
    "* ngram_range=(2,2) means look for 2-grams, not 1-grams\n",
    "* min_df=3 : Means include any term that occurs i requires a 2-gram to appear in at least 3 documents in train set\n",
    "* max_df=0.5 : Means include any terms that appears in less than 50% (fraction of 0.5) of training docs\n",
    "* binary=False : means it will do counts, not just binary presence/absence of each vocab term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_preprocessor = CountVectorizer(binary=False, ngram_range=(2,2), min_df=3, max_df=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.75, min_df=3, ngram_range=(2, 2))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_preprocessor.fit(list_of_training_text_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_gram_preprocessor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 106 waste of\n",
      "  17 does not\n",
      "  93 to use\n",
      "  56 of the\n",
      " 111 with the\n",
      "  32 in my\n",
      "  94 very disappointed\n",
      "  81 the service\n",
      "  69 service was\n",
      " 105 was very\n",
      "  41 is the\n",
      "  29 had to\n",
      "  10 and the\n",
      "  70 that it\n",
      "  45 it is\n",
      "  80 the same\n",
      "  16 do not\n",
      " 110 with my\n",
      "  47 like the\n",
      "  19 felt like\n",
      "  12 and they\n",
      "  89 this product\n",
      "  65 recommend this\n",
      "  86 this item\n",
      "  91 to anyone\n",
      "  53 not impressed\n",
      "  54 not work\n",
      "  33 in the\n",
      "  78 the phone\n",
      "  85 this is\n",
      "  60 piece of\n",
      "  55 of junk\n",
      "  75 the ear\n",
      "  51 my ear\n",
      " 113 you are\n",
      "  87 this phone\n",
      "  76 the first\n",
      "  72 the battery\n",
      "  92 to be\n",
      "  15 but it\n",
      "  21 for me\n",
      "  57 on the\n",
      " 104 was the\n",
      "  13 and was\n",
      " 102 was not\n",
      "  30 have to\n",
      "   8 and service\n",
      "  42 is very\n",
      "  39 is not\n",
      "  52 my phone\n",
      "  83 there is\n",
      "  44 it in\n",
      "  59 phone for\n",
      "  82 the worst\n",
      " 115 you have\n",
      "  22 for the\n",
      "   7 and it\n",
      "   0 about this\n",
      "  46 it was\n",
      " 109 which is\n",
      "  48 ll be\n",
      "  11 and then\n",
      "   9 and so\n",
      " 114 you get\n",
      "  74 the company\n",
      "   1 all in\n",
      "  31 in all\n",
      "  18 doesn work\n",
      "  43 it and\n",
      "  14 because it\n",
      "   5 and had\n",
      "  28 had the\n",
      "  38 is great\n",
      "  88 this place\n",
      "  40 is perfect\n",
      "   4 and delicious\n",
      "   3 an excellent\n",
      "  27 great service\n",
      "  63 portions and\n",
      "  77 the food\n",
      "  20 food was\n",
      "  61 place is\n",
      "  58 our server\n",
      "  66 server was\n",
      " 100 was great\n",
      "  24 great and\n",
      "   2 all the\n",
      "  90 time and\n",
      " 112 wonderful and\n",
      "  79 the place\n",
      "  67 service and\n",
      "  98 was fantastic\n",
      "  35 in vegas\n",
      "  25 great food\n",
      "  49 love the\n",
      "  68 service is\n",
      " 107 we were\n",
      "  64 really good\n",
      "  23 good and\n",
      "  34 in town\n",
      "  36 is an\n",
      "  71 the atmosphere\n",
      "  73 the best\n",
      "  26 great place\n",
      "  62 place to\n",
      " 108 were great\n",
      "  50 loved the\n",
      "  37 is good\n",
      "  99 was good\n",
      "  96 waitress was\n",
      "  84 they have\n",
      "  95 very good\n",
      " 101 was my\n",
      " 103 was so\n",
      "  97 was delicious\n",
      "   6 and happy\n"
     ]
    }
   ],
   "source": [
    "for term, count in two_gram_preprocessor.vocabulary_.items():\n",
    "    print(\"%4d %s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: A Bag-of-Words CountVectorizer + Classifier pipeline\n",
    "\n",
    "If we do a *pipeline*, we can make sure that:\n",
    "\n",
    "1) We apply the same text transforms to training and text data\n",
    "\n",
    "2) We use nice sklearn functionality that you don't need to reinvent from scratch\n",
    "\n",
    "In this part, we'll show you how to combine CountVectorizer with a classifier, and do a *combined* grid search over relevant hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline : CountVectors + classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bow_classifier_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,1))),\n",
    "    ('my_classifier', sklearn.linear_model.LogisticRegression(C=1.0, max_iter=20, random_state=101)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparam grid to search\n",
    "\n",
    "Remember, for a *pipeline*, the name of each parameter is built by concatenating string names, like\n",
    "\n",
    "```\n",
    "<step_name>_<hyperparameter_name>\n",
    "```\n",
    "\n",
    "* where <step_name> is name of the step in the pipeline, like \"my_classifier\", which we defined when we created the pipeline\n",
    "* where <hyperparameter_name> is the name of the hyperparameter for that step, like \"C\" which is a known parameter for that step's sklearn class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parameter_grid_by_name = dict()\n",
    "my_parameter_grid_by_name['my_bow_feature_extractor__min_df'] = [1, 2, 4]\n",
    "my_parameter_grid_by_name['my_classifier__C'] = np.logspace(-4, 4, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scoring_metric_name = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a \"predefined\" split of our training data into train and validation (so we can do grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training text reviews and labels\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the training labels have STRUCTURE, so we better scramble them when we define our split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_tr_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 400 total examples. Let's randomly assign 100 to validation and 300 to training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(0)\n",
    "\n",
    "valid_ids = prng.choice(np.arange(N), size=100)\n",
    "\n",
    "valid_indicators_N = np.zeros(N)\n",
    "valid_indicators_N[valid_ids] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom searcher object with all our settings in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher = sklearn.model_selection.GridSearchCV(\n",
    "    my_bow_classifier_pipeline,\n",
    "    my_parameter_grid_by_name,\n",
    "    scoring=my_scoring_metric_name,\n",
    "    cv=my_splitter,\n",
    "    refit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the grid search\n",
    "\n",
    "Remember, we expect some convergence warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mhughes/miniconda3/envs/comp135_2020f_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1,  0])),\n",
       "             estimator=Pipeline(steps=[('my_bow_feature_extractor',\n",
       "                                        CountVectorizer()),\n",
       "                                       ('my_classifier',\n",
       "                                        LogisticRegression(max_iter=20,\n",
       "                                                           random_state=101))]),\n",
       "             param_grid={'my_bow_feature_extractor__min_df': [1, 2, 4],\n",
       "                         'my_classifier__C': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,\n",
       "       1.e+04])},\n",
       "             refit=False, scoring='accuracy')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searcher.fit(list_of_training_text_reviews, y_tr_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch_results_df = pd.DataFrame(grid_searcher.cv_results_).copy()\n",
    "\n",
    "param_keys = ['param_my_bow_feature_extractor__min_df', 'param_my_classifier__C']\n",
    "\n",
    "# Rearrange row order so it is easy to skim\n",
    "gsearch_results_df.sort_values(param_keys, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results of grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_my_bow_feature_extractor__min_df</th>\n",
       "      <th>param_my_classifier__C</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.501587</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.720635</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.787302</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.501587</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.726984</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.726984</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.701587</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_my_bow_feature_extractor__min_df param_my_classifier__C  \\\n",
       "0                                       1                 0.0001   \n",
       "1                                       1                  0.001   \n",
       "2                                       1                   0.01   \n",
       "3                                       1                    0.1   \n",
       "4                                       1                      1   \n",
       "5                                       1                     10   \n",
       "6                                       1                    100   \n",
       "7                                       1                   1000   \n",
       "8                                       1                  10000   \n",
       "9                                       2                 0.0001   \n",
       "10                                      2                  0.001   \n",
       "11                                      2                   0.01   \n",
       "12                                      2                    0.1   \n",
       "13                                      2                      1   \n",
       "14                                      2                     10   \n",
       "15                                      2                    100   \n",
       "16                                      2                   1000   \n",
       "17                                      2                  10000   \n",
       "18                                      4                 0.0001   \n",
       "19                                      4                  0.001   \n",
       "20                                      4                   0.01   \n",
       "21                                      4                    0.1   \n",
       "22                                      4                      1   \n",
       "23                                      4                     10   \n",
       "24                                      4                    100   \n",
       "25                                      4                   1000   \n",
       "26                                      4                  10000   \n",
       "\n",
       "    split0_test_score  rank_test_score  \n",
       "0            0.498413               24  \n",
       "1            0.501587               22  \n",
       "2            0.720635               14  \n",
       "3            0.777778                7  \n",
       "4            0.777778                7  \n",
       "5            0.793651                1  \n",
       "6            0.790476                3  \n",
       "7            0.793651                1  \n",
       "8            0.787302                5  \n",
       "9            0.498413               24  \n",
       "10           0.501587               22  \n",
       "11           0.704762               17  \n",
       "12           0.761905                9  \n",
       "13           0.780952                6  \n",
       "14           0.790476                3  \n",
       "15           0.746032               10  \n",
       "16           0.726984               12  \n",
       "17           0.714286               15  \n",
       "18           0.498413               24  \n",
       "19           0.498413               24  \n",
       "20           0.688889               19  \n",
       "21           0.726984               12  \n",
       "22           0.736508               11  \n",
       "23           0.711111               16  \n",
       "24           0.701587               18  \n",
       "25           0.682540               20  \n",
       "26           0.676190               21  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch_results_df[param_keys + ['split0_test_score', 'rank_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 6a: Which settings of min_df seem to perform best? Why do you think that is?\n",
    "\n",
    "Hint: Which settings produce more features and which produce less? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise 6b: Do 2-grams perform better than 1-grams?\n",
    "\n",
    "Can you run a grid search to find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
